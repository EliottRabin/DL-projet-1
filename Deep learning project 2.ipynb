{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for NLP - Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import numpy as np\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DATA = \"./data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Monolingual (English) word embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "class Word2vec():\n",
    "    def __init__(self, fname, nmax=100000):\n",
    "        self.word2vec = self.load_wordvec(fname, nmax)\n",
    "        self.word2id = {word: i for i, word in enumerate(self.word2vec.keys())}\n",
    "        self.id2word = {v: k for k, v in self.word2id.items()}\n",
    "        self.embeddings = np.array(list(self.word2vec.values()))\n",
    "    \n",
    "    def load_wordvec(self, fname, nmax):\n",
    "        word2vec = {}\n",
    "        with io.open(fname, encoding='utf-8') as f:\n",
    "            next(f)\n",
    "            for i, line in enumerate(f):\n",
    "                word, vec = line.split(' ', 1)\n",
    "                word2vec[word] = np.fromstring(vec, sep=' ')\n",
    "                if i == (nmax - 1):\n",
    "                    break\n",
    "        print('Loaded %s pretrained word vectors' % (len(word2vec)))\n",
    "        return word2vec\n",
    "    \n",
    "    def most_similar(self, w: str, K:int =5) -> List:\n",
    "        \"\"\"Given a word, returns the K words most similar to that word\"\"\"\n",
    "        v = self.embeddings[self.word2id[w]]\n",
    "        most_similar = []\n",
    "        most_similar_indexes = np.argsort(self._score(v, self.embeddings))[::-1][1:K+1]\n",
    "        for index in most_similar_indexes:\n",
    "            most_similar.append(self.id2word[index])\n",
    "        # K most similar words: self.score  -  np.argsort \n",
    "        return most_similar\n",
    "\n",
    "    def score(self, w1: str, w2: str) -> np.float:\n",
    "        \"\"\" Given two words, returns the cosine similarity between their representation in word2vec\"\"\"\n",
    "        v1 = self.embeddings[self.word2id[w1]]\n",
    "        v2 = self.embeddings[self.word2id[w2]]\n",
    "        return self._score(v1, v2)\n",
    "    \n",
    "    def _score(self, v1: np.ndarray, v2: np.ndarray) -> np.float:\n",
    "        \"\"\"Given two np.ndarray of the same size returns the cosine similarity\"\"\"\n",
    "        # cosine similarity: np.dot  -  np.linalg.norm\n",
    "        if v1.ndim == 1 and v2.ndim == 1:\n",
    "            return np.dot(v1, v2) / (np.linalg.norm(v1)*np.linalg.norm(v2))\n",
    "        else:\n",
    "            if v1.ndim == 1:\n",
    "                v1 = v1.reshape((1,-1))\n",
    "            return (np.dot(v1, v2.T) / ((np.linalg.norm(v1, axis=1)*np.linalg.norm(v2, axis=1)) + 1e-12)).reshape((-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 100000 pretrained word vectors\n",
      "cat dog 0.671683666279249\n",
      "dog pet 0.6842064029669219\n",
      "dogs cats 0.7074389328052404\n",
      "paris france 0.7775108541288563\n",
      "germany berlin 0.7420295235998394\n",
      "['cats', 'kitty', 'kitten', 'feline', 'kitties']\n",
      "['dogs', 'puppy', 'Dog', 'doggie', 'canine']\n",
      "['dog', 'pooches', 'Dogs', 'doggies', 'canines']\n",
      "['france', 'Paris', 'london', 'berlin', 'tokyo']\n",
      "['austria', 'europe', 'german', 'berlin', 'poland']\n"
     ]
    }
   ],
   "source": [
    "w2v = Word2vec(os.path.join(PATH_TO_DATA, 'crawl-300d-200k.vec'), nmax=1e5)\n",
    "\n",
    "# You will be evaluated on the output of the following:\n",
    "for w1, w2 in zip(('cat', 'dog', 'dogs', 'paris', 'germany'), ('dog', 'pet', 'cats', 'france', 'berlin')):\n",
    "    print(w1, w2, w2v.score(w1, w2))\n",
    "for w1 in ['cat', 'dog', 'dogs', 'paris', 'germany']:\n",
    "    print(w2v.most_similar(w1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoV():\n",
    "    def __init__(self, w2v):\n",
    "        self.w2v = w2v\n",
    "        \n",
    "    def encode(self, sentences, idf=None):\n",
    "        # takes a list of sentences, outputs a numpy array of sentence embeddings\n",
    "        # see TP1 for help\n",
    "        sentemb = []\n",
    "        for i,sent in enumerate(sentences):\n",
    "            emb = []\n",
    "            sum_weight = 0\n",
    "            for word in sent.split():\n",
    "                # we are not sure that every words are in word2vec\n",
    "                if word in w2v.word2vec:\n",
    "                    vector = self.w2v.word2vec[word]\n",
    "                    if idf is not None and word in idf:\n",
    "                        weight = idf[word]\n",
    "                    else:\n",
    "                        weight = 1\n",
    "                    sum_weight += weight\n",
    "                    vector = vector*weight\n",
    "                    emb.append(vector)\n",
    "            if len(emb) > 0:\n",
    "                sentemb.append(np.sum(emb, axis=0)/sum_weight)\n",
    "            else:\n",
    "                sentemb.append(np.zeros((1,300)))\n",
    "        return np.vstack(sentemb)\n",
    "\n",
    "    def most_similar(self, s, sentences, idf=None, K=5):\n",
    "        # get most similar sentences and **print** them\n",
    "        vector = self.encode([s], idf)\n",
    "        sentences_vectors = self.encode(sentences, idf) \n",
    "        most_similar = []\n",
    "        most_similar_indexes = np.argsort(self.w2v._score(vector, sentences_vectors))[::-1][1:K+1]\n",
    "        for index in most_similar_indexes:\n",
    "            most_similar.append(sentences[index])\n",
    "        # K most similar words: self.score  -  np.argsort \n",
    "        return most_similar\n",
    "\n",
    "    def score(self, s1, s2, idf=None):\n",
    "        if isinstance(s1, str):\n",
    "            s1 = self.encode([s1], idf)\n",
    "        if isinstance(s2, str):\n",
    "            s2 = self.encode([s2], idf)\n",
    "        return self.w2v._score(s1, s2)\n",
    "    \n",
    "    def build_idf(self, sentences):\n",
    "        # build the idf dictionary: associate each word to its idf value\n",
    "        idf = {}\n",
    "        for sent in sentences:\n",
    "             for w in set(sent.split()):\n",
    "                idf[w] = idf.get(w, 0) + 1\n",
    "        \n",
    "        for word in idf:\n",
    "            idf[word] = max(1, np.log10(len(sentences) / idf[word]))\n",
    "        return idf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5000 pretrained word vectors\n",
      "1 smiling african american boy . \n",
      "With word2vec\n",
      "['blond boy waterskiing . ', 'a boy jumps . ', 'a boy jumps . ', 'a boy smiles underwater . ', 'a boy skateboarding . ']\n",
      "[0.60894451]\n",
      "\n",
      "With idf weighted\n",
      "['5 women and 1 man are smiling for the camera . ', 'a man rides a 4 wheeler in the desert . ', '3 males and 1 woman enjoying a sporting event ', 'a man in black is juggling 3 flamed bottles . ', '2 females , 1 from germany and 1 from china , compete in a wrestling match on a mat . ']\n",
      "[0.59633357]\n"
     ]
    }
   ],
   "source": [
    "w2v = Word2vec(os.path.join(PATH_TO_DATA, 'crawl-300d-200k.vec'), nmax=5000)\n",
    "s2v = BoV(w2v)\n",
    "\n",
    "# Load sentences in \"PATH_TO_DATA/sentences.txt\"\n",
    "with open(os.path.join(PATH_TO_DATA, 'sentences.txt'), encoding='utf-8') as f:\n",
    "    sentences = f.readlines()\n",
    "sentences = [sentence.replace('\\n', '') for sentence in sentences]\n",
    "\n",
    "# Build idf scores for each word\n",
    "idf = s2v.build_idf(sentences)\n",
    "\n",
    "# You will be evaluated on the output of the following:\n",
    "print(sentences[10])\n",
    "print('With word2vec')\n",
    "print(s2v.most_similar(sentences[10], sentences))  # BoV-mean\n",
    "print(s2v.score(sentences[7], sentences[13]))\n",
    "\n",
    "print('\\nWith idf weighted')\n",
    "print(s2v.most_similar(sentences[10], sentences, idf))  # BoV-idf\n",
    "print(s2v.score(sentences[7], sentences[13], idf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Multilingual (English-French) word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 50000 pretrained word vectors\n",
      "Loaded 50000 pretrained word vectors\n"
     ]
    }
   ],
   "source": [
    "# 1 - Download and load 50k first vectors of\n",
    "#     https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.en.vec\n",
    "#     https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.fr.vec\n",
    "\n",
    "w2v_fr = Word2vec(os.path.join(PATH_TO_DATA, 'wiki.fr.vec'), nmax=5e4)\n",
    "w2v_en = Word2vec(os.path.join(PATH_TO_DATA, 'wiki.en.vec'), nmax=5e4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 - Get words that appear in both vocabs (= identical character strings)\n",
    "#     Use it to create the matrix X and Y (of aligned embeddings for these words)\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "for i, word in enumerate(set(w2v_en.word2id.keys()).intersection(w2v_fr.word2id.keys())):\n",
    "    X.append(w2v_fr.embeddings[w2v_fr.word2id[word]])\n",
    "    Y.append(w2v_en.embeddings[w2v_en.word2id[word]])\n",
    "X = np.vstack(X).T\n",
    "Y = np.vstack(Y).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 - Solve the Procrustes using the scipy package and: scipy.linalg.svd() and get the optimal W\n",
    "#     Now W*French_vector is in the same space as English_vector\n",
    "\n",
    "U, S, V_T = np.linalg.svd(np.dot(Y,X.T))\n",
    "W = np.dot(U, V_T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From french to english\n",
      "\n",
      "pays ['countries', 'netherlands', 'country', 'luxembourg', 'wallonia']\n",
      "naissance ['birth_place', 'death_date', 'birth_date', 'death_place', 'location_city']\n",
      "souvent ['often', 'sometimes', 'usually', 'oftentimes', 'invariably']\n",
      "directeur ['director', 'assistant', 'consultant', 'directorship', 'adviser']\n",
      "films ['films', 'movies', 'film', 'movie', 'soundtracks']\n",
      "réseau ['network', 'networks', 'interconnection', 'interconnect', 'connections']\n",
      "ville ['city', 'town', 'cities', 'suburb', 'towns']\n",
      "chaise ['rope', 'door', 'hanger', 'stretcher', 'rack']\n",
      "\n",
      "From english to french\n",
      "country ['country', 'countries', 'swaziland', 'pays', 'england']\n",
      "birth ['accouchement', 'birth', 'grossesse', 'naissance', 'naissances']\n",
      "bird ['birds', 'bird', 'oiseau', 'oiseaux', 'busard']\n",
      "teacher ['teacher', 'enseignante', 'enseigner', 'enseignait', 'enseignant']\n",
      "try ['users', 'essaye', 'read', 'essayer', 'drop']\n",
      "lol ['lol', 'ouais', '^^', 'bla', 'poulpy']\n",
      "door ['door', 'portes', 'banquette', 'wagon', 'coffre']\n"
     ]
    }
   ],
   "source": [
    "# 4 - After alignment with W, give examples of English nearest neighbors of some French words (and vice versa)\n",
    "#     You will be evaluated on that part and the code above\n",
    "\n",
    "# French embedding\n",
    "fr_emb = w2v_fr.embeddings.T\n",
    "# English embedding\n",
    "en_emb = w2v_en.embeddings.T\n",
    "# English embedding approximated from french embedding\n",
    "approx_en_emb = np.dot(W, fr_emb)\n",
    "approx_fr_emb = np.dot(W.T, en_emb)\n",
    "\n",
    "print('From french to english\\n')\n",
    "for word in ['pays', 'naissance', 'souvent', 'directeur', 'films', 'réseau', 'ville', 'chaise']:\n",
    "    word_en_approx = approx_en_emb[:, w2v_fr.word2id[word]]\n",
    "    similarity = w2v._score(word_en_approx, en_emb.T)\n",
    "\n",
    "    most_similar = []\n",
    "    for index in np.argsort(similarity)[::-1][:5]:\n",
    "        most_similar.append(w2v_en.id2word[index])\n",
    "    print(word, most_similar)\n",
    "    \n",
    "print('\\nFrom english to french')\n",
    "for word in ['country', 'birth', 'bird', 'teacher', 'try', 'lol', 'door']:\n",
    "    word_fr_approx = approx_fr_emb[:, w2v_en.word2id[word]]\n",
    "    similarity = w2v._score(word_fr_approx, fr_emb.T)\n",
    "\n",
    "    most_similar = []\n",
    "    for index in np.argsort(similarity)[::-1][:5]:\n",
    "        most_similar.append(w2v_fr.id2word[index])\n",
    "    print(word, most_similar)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to dive deeper on this subject: https://github.com/facebookresearch/MUSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Sentence classification with BoV and scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 - Load train/dev/test of Stanford Sentiment TreeBank (SST)\n",
    "#     (https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf)\n",
    "\n",
    "def load_dataset(fname: str , test_set=False) -> np.ndarray:\n",
    "    with io.open(fname, encoding='utf-8') as f:\n",
    "        if not test_set:\n",
    "            labels = []\n",
    "            sentences = []\n",
    "            for i, line in enumerate(f):\n",
    "                label, sentence = line.split(' ', 1)\n",
    "                labels.append(label)\n",
    "                sentences.append(sentence.replace('\\n', ''))\n",
    "            return sentences, labels\n",
    "        else:\n",
    "            return [sentence.replace('\\n', '') for sentence in f.readlines()], []\n",
    "        \n",
    "train_data, y_train = load_dataset(os.path.join(PATH_TO_DATA, 'SST/stsa.fine.train'))\n",
    "dev_data, y_dev = load_dataset(os.path.join(PATH_TO_DATA, 'SST/stsa.fine.dev'))\n",
    "test_data, _ = load_dataset(os.path.join(PATH_TO_DATA, 'SST/stsa.fine.test.X'), test_set=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 - Encode sentences with the BoV model above\n",
    "train = s2v.encode(train_data)\n",
    "dev = s2v.encode(dev_data)\n",
    "test = s2v.encode(test_data)\n",
    "\n",
    "idf = s2v.build_idf(train_data)\n",
    "train_idf = s2v.encode(train_data, idf)\n",
    "dev_idf = s2v.encode(dev_data, idf)\n",
    "test_idf = s2v.encode(test_data, idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train LogisticRegression on word2vec\n",
      "\n",
      "Fitting the penalisation on accuracy score {0.1: 0.36239782016348776, 0.5: 0.3851044504995459, 1: 0.3723887375113533, 5: 0.3814713896457766, 100.0: 0.3751135331516803, 500.0: 0.37420526793823794, 1000.0: 0.37783832879200724, 5000.0: 0.3796548592188919, 10000.0: 0.3787465940054496, 100000.0: 0.3787465940054496, 1000000.0: 0.3796548592188919, 10000000.0: 0.3760217983651226, 100000000.0: 0.3787465940054496}\n",
      "\n",
      "Accuracy score on dev with LogisticRegression on word2vec:  0.3851044504995459\n",
      "========================================\n",
      "Train LogisticRegression on word2vec idf weighted\n",
      "\n",
      "Fitting the penalisation on accuracy score {0.1: 0.36966394187102636, 0.5: 0.3896457765667575, 1: 0.3787465940054496, 5: 0.35876475930971846, 100.0: 0.35149863760217986, 500.0: 0.3505903723887375, 1000.0: 0.35240690281562215, 5000.0: 0.3533151680290645, 10000.0: 0.3569482288828338, 100000.0: 0.3505903723887375, 1000000.0: 0.3542234332425068, 10000000.0: 0.3505903723887375, 100000000.0: 0.35149863760217986}\n",
      "\n",
      "Accuracy score on dev with LogisticRegression on word2vec idf weighted:  0.3896457765667575\n"
     ]
    }
   ],
   "source": [
    "# 3 - Learn Logistic Regression on top of sentence embeddings using scikit-learn\n",
    "#     (consider tuning the L2 regularization on the dev set)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "def train_model(train, y_train, dev, y_dev, model_name='word2vec'):\n",
    "    print(f'Train LogisticRegression on {model_name}\\n')\n",
    "    accuracy_scores = {}\n",
    "    for C in [1e-1, 5e-1, 1, 5, 1e2, 5e2, 1e3, 5e3, 1e4, 1e5, 1e6, 1e7, 1e8]:\n",
    "        clf = LogisticRegression(C=C, random_state=0, solver='lbfgs', multi_class='multinomial', n_jobs=-1)\n",
    "        clf.fit(train, y_train)\n",
    "        y_pred = clf.predict(dev)\n",
    "        accuracy_scores[C] = accuracy_score(y_dev, y_pred)\n",
    "    print('Fitting the penalisation on accuracy score', accuracy_scores)\n",
    "    C = max([(acc, C) for C, acc in accuracy_scores.items()])[1]\n",
    "    clf = LogisticRegression(C=C, random_state=0, solver='lbfgs', multi_class='multinomial', n_jobs=-1,)\n",
    "    clf.fit(train, y_train)\n",
    "    print(f'\\nAccuracy score on dev with LogisticRegression on {model_name}: ', accuracy_score(y_dev, clf.predict(dev)))\n",
    "    return clf\n",
    "\n",
    "clf = train_model(train, y_train, dev, y_dev, model_name='word2vec')\n",
    "print('========================================')\n",
    "clf_idf = train_model(train_idf, y_train, dev_idf, y_dev, model_name='word2vec idf weighted')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 - Produce 2210 predictions for the test set (in the same order). One line = one prediction (=0,1,2,3,4).\n",
    "#     Attach the output file \"logreg_bov_y_test_sst.txt\" to your deliverable.\n",
    "#     You will be evaluated on the results of the test set.\n",
    "y_pred = clf_idf.predict(test_idf)\n",
    "with open(os.path.join(PATH_TO_DATA, 'logreg_bov_y_test_sst.txt'), 'w') as f:\n",
    "    f.writelines('\\n'.join(y_pred.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train svm on word2vec\n",
      "Fitting the penalisation on accuracy score {0.1: 0.3851044504995459, 0.5: 0.3787465940054496, 1: 0.38419618528610355, 5: 0.38056312443233425}\n",
      "Train svm on word2vec idf weighted\n",
      "Fitting the penalisation on accuracy score {0.1: 0.3723887375113533, 0.5: 0.38056312443233425, 1: 0.3760217983651226, 5: 0.37420526793823794}\n"
     ]
    }
   ],
   "source": [
    "# BONUS!\n",
    "# 5 - Try to improve performance with another classifier\n",
    "#     Attach the output file \"XXX_bov_y_test_sst.txt\" to your deliverable (where XXX = the name of the classifier)\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "def train_svm(train, y_train, dev, y_dev, data_name):\n",
    "    print(f'Train svm on {data_name}')\n",
    "    accuracy_scores = {}\n",
    "    models = {}\n",
    "    for C in [1e-1, 5e-1, 1, 5]:\n",
    "        svm = LinearSVC(C=C, max_iter=5000)\n",
    "        svm.fit(train, y_train)\n",
    "        y_pred = svm.predict(dev)\n",
    "        accuracy_scores[C] = accuracy_score(y_dev, y_pred)\n",
    "        models[C] = svm\n",
    "    print('Fitting the penalisation on accuracy score', accuracy_scores)\n",
    "    C = max([(acc, C) for C, acc in accuracy_scores.items()])[1]\n",
    "    svm = models[C]\n",
    "    return svm\n",
    "svm = train_svm(train, y_train, dev, y_dev, 'word2vec')\n",
    "svm_idf = train_svm(train_idf, y_train, dev_idf, y_dev, 'word2vec idf weighted')\n",
    "y_pred = svm.predict(test)\n",
    "with open(os.path.join(PATH_TO_DATA, 'XXX_bov_y_test_sst.txt'), 'w') as f:\n",
    "    f.writelines('\\n'.join(y_pred.tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Sentence classification with LSTMs in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 - Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 - Load train/dev/test sets of SST\n",
    "\n",
    "train, y_train = load_dataset(os.path.join(PATH_TO_DATA, 'SST/stsa.fine.train'))\n",
    "dev, y_dev = load_dataset(os.path.join(PATH_TO_DATA, 'SST/stsa.fine.dev'))\n",
    "test, _ = load_dataset(os.path.join(PATH_TO_DATA, 'SST/stsa.fine.test.X'), test_set=True)\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes=5)\n",
    "y_dev = keras.utils.to_categorical(y_dev, num_classes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 - Transform text to integers using keras.preprocessing.text.one_hot function\n",
    "#     https://keras.io/preprocessing/text/\n",
    "words = set()\n",
    "for sentence in train:\n",
    "    words.update(set(sentence.split()))\n",
    "n = len(words)\n",
    "\n",
    "def one_hot(sentences):\n",
    "    encoded_sentences = []\n",
    "    for sentence in sentences:\n",
    "        encoded_sentences.append(keras.preprocessing.text.one_hot(sentence, n))\n",
    "    return encoded_sentences\n",
    "train_sentences = one_hot(train)\n",
    "test_sentences = one_hot(test)\n",
    "dev_sentences = one_hot(dev)\n",
    "maxseqlen = max([len(sentence) for sentence in train_sentences + test_sentences + dev_sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 - Pad your sequences using keras.preprocessing.sequence.pad_sequences\n",
    "#     https://keras.io/preprocessing/sequence/\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "train_sentences = np.vstack(train_sentences)\n",
    "train_sentences = pad_sequences(train_sentences, maxlen=maxseqlen, value=0.0)\n",
    "train_sentences = np.vstack(train_sentences)\n",
    "\n",
    "test_sentences = pad_sequences(test_sentences, maxlen=maxseqlen, value=0.0)\n",
    "test_sentences = np.vstack(test_sentences)\n",
    "\n",
    "dev_sentences = pad_sequences(dev_sentences, maxlen=maxseqlen, value=0.0)\n",
    "dev_sentences = np.vstack(dev_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 - Design and train your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mnannan/.virtualenvs/supelec/lib/python3.6/site-packages/ipykernel_launcher.py:20: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(128, dropout=0.5, recurrent_dropout=0.5)`\n"
     ]
    }
   ],
   "source": [
    "# 4 - Design your encoder + classifier using keras.layers\n",
    "#     In Keras, Torch and other deep learning framework, we create a \"container\" which is the Sequential() module.\n",
    "#     Then we add components to this contained : the lookuptable, the LSTM, the classifier etc.\n",
    "#     All of these components are contained in the Sequential() and are trained together.\n",
    "\n",
    "\n",
    "# ADAPT CODE BELOW\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Activation \n",
    "\n",
    "embed_dim  = 128  # word embedding dimension\n",
    "nhid       = 128  # number of hidden units in the LSTM\n",
    "vocab_size = n  # size of the vocabulary\n",
    "n_classes  = 5\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embed_dim, input_length=maxseqlen))\n",
    "model.add(LSTM(nhid, dropout_W=0.5, dropout_U=0.5))\n",
    "model.add(Dense(n_classes, activation='sigmoid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_20 (Embedding)     (None, 52, 128)           2122112   \n",
      "_________________________________________________________________\n",
      "lstm_20 (LSTM)               (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 2,254,341\n",
      "Trainable params: 2,254,341\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# 5 - Define your loss/optimizer/metrics\n",
    "\n",
    "loss_classif     =  'categorical_crossentropy' # find the right loss for multi-class classification\n",
    "optimizer        =  'adam' # find the right optimizer\n",
    "metrics_classif  =  ['accuracy']\n",
    "\n",
    "# Observe how easy (but blackboxed) this is in Keras\n",
    "model.compile(loss=loss_classif,\n",
    "              optimizer=optimizer,\n",
    "              metrics=metrics_classif)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mnannan/.virtualenvs/supelec/lib/python3.6/site-packages/ipykernel_launcher.py:7: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8544 samples, validate on 1101 samples\n",
      "Epoch 1/10\n",
      "8544/8544 [==============================] - 17s 2ms/step - loss: 1.5721 - acc: 0.2748 - val_loss: 1.5597 - val_acc: 0.2752\n",
      "Epoch 2/10\n",
      "8544/8544 [==============================] - 13s 2ms/step - loss: 1.4557 - acc: 0.3613 - val_loss: 1.3995 - val_acc: 0.3724\n",
      "Epoch 3/10\n",
      "8544/8544 [==============================] - 13s 2ms/step - loss: 1.2150 - acc: 0.4486 - val_loss: 1.4241 - val_acc: 0.3851\n",
      "Epoch 4/10\n",
      "8544/8544 [==============================] - 13s 1ms/step - loss: 1.0277 - acc: 0.5183 - val_loss: 1.5579 - val_acc: 0.3933\n",
      "Epoch 5/10\n",
      "8544/8544 [==============================] - 14s 2ms/step - loss: 0.8741 - acc: 0.6203 - val_loss: 1.6920 - val_acc: 0.3760\n",
      "Epoch 6/10\n",
      "8544/8544 [==============================] - 14s 2ms/step - loss: 0.7229 - acc: 0.7130 - val_loss: 1.8279 - val_acc: 0.3842\n",
      "Epoch 7/10\n",
      "8544/8544 [==============================] - 15s 2ms/step - loss: 0.5947 - acc: 0.7776 - val_loss: 2.0169 - val_acc: 0.3824\n",
      "Epoch 8/10\n",
      "8544/8544 [==============================] - 15s 2ms/step - loss: 0.4732 - acc: 0.8276 - val_loss: 2.2760 - val_acc: 0.3697\n",
      "Epoch 9/10\n",
      "8544/8544 [==============================] - 14s 2ms/step - loss: 0.4044 - acc: 0.8549 - val_loss: 2.3725 - val_acc: 0.3515\n",
      "Epoch 10/10\n",
      "8544/8544 [==============================] - 15s 2ms/step - loss: 0.3437 - acc: 0.8791 - val_loss: 2.6891 - val_acc: 0.3524\n"
     ]
    }
   ],
   "source": [
    "# 6 - Train your model and find the best hyperparameters for your dev set\n",
    "#     you will be evaluated on the quality of your predictions on the test set\n",
    "\n",
    "bs = 64\n",
    "n_epochs = 10\n",
    "\n",
    "history = model.fit(train_sentences, y_train, batch_size=bs, nb_epoch=n_epochs, validation_data=(dev_sentences, y_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.scatter(history.epoch, history.history['acc'], marker='+')\n",
    "plt.scatter(history.epoch, history.history['val_acc'], marker='+', color='r')\n",
    "plt.legend(['Train accuracy', 'Validation accuracy'])\n",
    "plt.title('Train and Validation accuracy during the training')\n",
    "plt.ylabel('Accuracy')\n",
    "_ = plt.xlabel('Epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mnannan/.virtualenvs/supelec/lib/python3.6/site-packages/ipykernel_launcher.py:7: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8544 samples, validate on 1101 samples\n",
      "Epoch 1/10\n",
      "8544/8544 [==============================] - 16s 2ms/step - loss: 1.5769 - acc: 0.2725 - val_loss: 1.5668 - val_acc: 0.2543\n",
      "Epoch 2/10\n",
      "8544/8544 [==============================] - 11s 1ms/step - loss: 1.5121 - acc: 0.3352 - val_loss: 1.4387 - val_acc: 0.3697\n",
      "Epoch 3/10\n",
      "8544/8544 [==============================] - 14s 2ms/step - loss: 1.2901 - acc: 0.4338 - val_loss: 1.4038 - val_acc: 0.3742\n",
      "Epoch 4/10\n",
      "8544/8544 [==============================] - 14s 2ms/step - loss: 1.1050 - acc: 0.5022 - val_loss: 1.4588 - val_acc: 0.3815\n",
      "Epoch 5/10\n",
      "8544/8544 [==============================] - 14s 2ms/step - loss: 0.9398 - acc: 0.5872 - val_loss: 1.5670 - val_acc: 0.3760\n",
      "Epoch 6/10\n",
      "8544/8544 [==============================] - 15s 2ms/step - loss: 0.7909 - acc: 0.6946 - val_loss: 1.7284 - val_acc: 0.3769\n",
      "Epoch 7/10\n",
      "8544/8544 [==============================] - 11s 1ms/step - loss: 0.6431 - acc: 0.7635 - val_loss: 1.9284 - val_acc: 0.3733\n",
      "Epoch 8/10\n",
      "8544/8544 [==============================] - 11s 1ms/step - loss: 0.5326 - acc: 0.8045 - val_loss: 2.1381 - val_acc: 0.3751\n",
      "Epoch 9/10\n",
      "8544/8544 [==============================] - 10s 1ms/step - loss: 0.4469 - acc: 0.8370 - val_loss: 2.3074 - val_acc: 0.3533\n",
      "Epoch 10/10\n",
      "8544/8544 [==============================] - 12s 1ms/step - loss: 0.3950 - acc: 0.8580 - val_loss: 2.4812 - val_acc: 0.3597\n"
     ]
    }
   ],
   "source": [
    "# 6 - Train your model and find the best hyperparameters for your dev set\n",
    "#     you will be evaluated on the quality of your predictions on the test set\n",
    "\n",
    "bs = 64\n",
    "n_epochs = 10\n",
    "\n",
    "history = model.fit(train_sentences, y_train, batch_size=bs, nb_epoch=n_epochs, validation_data=(dev_sentences, y_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7 - Generate your predictions on the test set using model.predict(x_test)\n",
    "#     https://keras.io/models/model/\n",
    "#     Log your predictions in a file (one line = one integer: 0,1,2,3,4)\n",
    "#     Attach the output file \"logreg_lstm_y_test_sst.txt\" to your deliverable.\n",
    "\n",
    "with open(os.path.join(PATH_TO_DATA, 'logreg_lstm_y_test_sst.txt'), 'w') as f:\n",
    "    f.writelines('\\n'.join(np.argmax(model.predict(test_sentences), axis=1).astype('str').tolist()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 -- innovate !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 50000 pretrained word vectors\n"
     ]
    }
   ],
   "source": [
    "# 8 - Open question: find a model that is better on your dev set\n",
    "#     (e.g: use a 1D ConvNet, use a better classifier, pretrain your lookup tables ..)\n",
    "#     you will get point if the results on the test set are better: be careful of not overfitting your dev set too much..\n",
    "#     Attach the output file \"XXX_XXX_y_test_sst.txt\" to your deliverable.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
